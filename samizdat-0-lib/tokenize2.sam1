# Copyright 2013 the Samizdat Authors (Dan Bornstein et alia).
# Licensed AS IS and WITHOUT WARRANTY under the Apache License,
# Version 2.0. Details: <http://www.apache.org/licenses/LICENSE-2.0>

#
# Samizdat Layer 2 Tokenizer
#


# Forward declarations for Layer 2 definitions.
def tokMultilineComment = forwardFunction();
def tokStringPart2 = forwardFunction();


#
# This section is meant to mirror the code in the spec for tokenization as
# closely as possible, comments and all. The spec is a self-description of
# *Layer 1* parsing, and the comments should be taken in light of that.
#

# Map of all the keywords, from their string name to valueless tokens. These
# are (to a first approximation) operators whose spellings match the
# tokenization syntax of identifiers.
def KEYWORDS = mapFromGenerator(
    filterGenerator([
        "def", "fn", "return",
        # *Layer 2* defines additional keywords here.
        "break", "continue", "default", "do", "else", "export",
        "for", "if", "import", "in", "module", "reduce",
        "switch", "var", "while"])
        { name <> [(name): @[(name)]] });

# These are all the int digits, as a map from strings to digit values. This
# includes hex digits as well, in both lower and upper case. Finally, this
# includes a mapping of `"_"` to `-1` for the implementation of the
# "digit space" syntax.
#
# **Note:** Only the decimal digits matter in *Layer 0* and *Layer 1*.
def INT_CHARS = [
    "0": 0, "1": 1, "2": 2, "3": 3, "4": 4,
    "5": 5, "6": 6, "7": 7, "8": 8, "9": 9,
    "a": 10, "b": 11, "c": 12, "d": 13, "e": 14, "f": 15,
    "A": 10, "B": 11, "C": 12, "D": 13, "E": 14, "F": 15,
    "_": -1
];

# Given a decimal digit, returns the digit value.
fn intFromDigitChar(ch) {
    <> mapGet(INT_CHARS, typeOf(ch))
};

# Processes a list of `stringPart` elements, yielding a literal `string`
# value. In *Layer 2* (and higher) this can also yield an
# `interpolatedString` or an `error`.
def processStringParts = forwardFunction();

# Parses any amount of whitespace and comments (including nothing at all).
# **Note:** The yielded result is always ignored.
def tokOptWhitespace = {/
    (
        # The lookahead here is to avoid the bulk of this rule if there's
        # no chance we're actually looking at whitespace. The final
        # lookahead character is only useful as of *Layer 2*.
        &["# \n" "/"]

        (
            [" " "\n"]+
        |
            # The `?` at the end is to handle end-of-file comments.
            "#" [! "\n"]* "\n"?
        |
            # Introduced in *Layer 2*.
            &"/*" # Avoid calling the rule unless we know it will match.
            tokMultilineComment
        )*
    )?
/};

# Parses punctuation and operators.
#
# **Note:** This rule is expanded significantly in *Layer 2*.
def tokPunctuation = {/
    # The lookahead here is done to avoid bothering with the choice
    # expression, except when we have a definite match. The second
    # string in the lookahead calls out the characters that are only
    # needed in *Layer 1*. Yet more characters are included in *Layer 2*.
    &["&@:.,=-+?;*<>{}()[]" "/|!" "\\%^"]

    (
        # Introduced in *Layer 2*.
        "\\==" | "\\!=" | "\\<=" | "\\>=" | "\\<" | "\\>"
    |
        # Introduced in *Layer 2*.
        "&&&" | "|||" | "^^^" | "!!!" | "<<<" | ">>>" | "..!" | "..+"
    |
        # Introduced in *Layer 2*.
        "==" | "!=" | "<=" | ">=" | "??" | "**" | ":="
    |
        "::" | ".." | "<>"
    |
        # These are only needed in *Layer 1*.
        "{/" | "/}"
    |
        # Note: We check this error condition here instead of in the
        # comment parsing code, because comments get parsed as whitespace,
        # which gets ignored. Rather than changing to not-quite-ignore
        # whitespace -- which would be messy -- we instead notice `/*` here
        # where we're parsing other punctuation. This is clean but a little
        # non-obvious, hence this comment.
        "/*" .*
        { <> @[error: "Unterminated comment."] }
    |
        # Single-character punctuation / operator.
        .
    )
/};

# Parses an integer literal.
#
# **Note:** This rule is rewritten in *Layer 2*.
def tokInt = forwardFunction();

# Parses a run of regular characters or an escape / special sequence,
# inside a quoted string.
#
# **Note:** Additional rules for string character parsing are defined
# in *Layer 2*.
def tokStringPart = {/
    (
        chars = [! "\\" "\"" "\n"]+
        { <> stringFromTokenList(chars) }
    )
|
    # This is the rule that ignores spaces after newlines.
    "\n"
    " "*
    { <> "\n" }
|
    "\\"
    (
        "\\" { <> "\\" } |
        "\"" { <> "\"" } |
        "n"  { <> "\n" } |
        "r"  { <> "\r" } |
        "t"  { <> "\t" } |
        "0"  { <> "\0" }
    )
|
    # Introduced in *Layer 2*.
    tokStringPart2
/};

# Parses a quoted string.
def tokString = {/
    "\""
    parts = tokStringPart*

    (
        "\""
        { <> processStringParts(parts) }
    |
        { <> @[error: "Unterminated string literal."] }
    )
/};

# Parses an identifier (in the usual form). This also parses keywords.
def tokIdentifier = {/
    first = ["_" "a".."z" "A".."Z"]
    rest = ["_" "a".."z" "A".."Z" "0".."9"]*

    {
        def string = stringFromTokenList([first, rest*]);
        <> ifValueOr { <> mapGet(KEYWORDS, string) }
            { <> @[identifier: string] }
    }
/};

# Parses the quoted-string identifier form.
def tokQuotedIdentifier = {/
    "\\"
    s = tokString

    { <> @[identifier: dataOf(s)] }
/};

# "Parses" an unrecognized character. This also consumes any further characters
# on the same line, in an attempt to resynch the input.
def tokError = {/
    badCh = .
    [! "\n"]*

    {
        def msg = stringCat("Unrecognized character: ", typeOf(badCh));
        <> @[error: msg]
    }
/};

# Parses an arbitrary token or error.
def tokToken = {/
    tokString | tokIdentifier | tokQuotedIdentifier
|
    # This needs to be listed after the quoted identifier rule, to
    # prevent `\"...` from being treated as a `\` token followed by
    # a string.
    tokPunctuation
|
    # This needs to be listed after the identifier rule, to prevent
    # an identifier-initial `_` from triggering this rule. (This is
    # only significant in *Layer 2* and higher.)
    tokInt
|
    tokError
/};

# Parses a file of tokens, yielding a list of them.
def tokFile = {/
    tokens = (tokOptWhitespace tokToken)*
    tokOptWhitespace

    { <> tokens }
/};


#
# *Layer 2* definitions
#
# These are all specific to parsing *Layer 2* (and higher).
#

# Map from opening tokens to corresponding closers. Only includes mappings
# necessary to parse string interpolations.
def OPEN_CLOSE_MAP = [
    "(":  ")",
    "{":  "}",
    "{/": "/}"
];

# Converts a list of digit values into an int, given the base. If `baseSign`
# is negative, this indicates that the result should be negative.
fn intFromDigitList(baseSign, digits) {
    def base = iabs(baseSign);
    def value = doReduce1(digits, 0) { digit, result ::
        <> ifIs { <> eq(digit, -1) }
            { <> result }
            { <> iadd(imul(result, base), digit) }
    };

    <> imul(value, isign(baseSign))
};

fn implProcessStringParts(parts) {
    # Handle the empty string as a special case, to make the
    # reduce below be more straightforward.
    ifIs { <> eq(parts, []) }
        { return @[string: ""] };

    # If there is an error part (represented as an `@error` token), return
    # it.
    doFilter(parts)
        { part ::
            <> ifIs { <> eq(typeOf(part), "error") }
                { return part }
        };

    # Coalesce adjacent literal strings in `parts`. The result is an
    # `elems` list consisting of multi-character strings and
    # interpolation maps.
    def elems = doReduce1(listButFirst(parts), [listFirst(parts)])
        { part, result ::
            <> ifValue { <> [&isString(listLast(result)), &isString(part)] }
                { newLast <> [listButLast(result)*, stringCat(newLast*)] }
                { <> [result*, part] };
        };

    # Check for a simple result.
    ifIs { <> eq(coreSizeOf(elems), 1) }
        {
            ifValue { <> isString(elems*) }
                { elem ::
                    # There's only one element, and it's a string. So,
                    # we have a simple non-interpolated string. Return it.
                    return @[string: elem]
                }
        };

    # At this point, we have a valid interpolation.
    <> @[interpolatedString: elems]
};
processStringParts(implProcessStringParts);

# Parses a `/* ... */` comment, with nesting.
def implMultilineComment = {/
    "/*"

    (
        tokMultilineComment
    |
        [! "*"]
    |
        "*" !"/"
    )*

    "*/"
/};
tokMultilineComment(implMultilineComment);

# Parses a single binary digit (or spacer), returning its value.
def tokBinDigit = {/
    ch = ["_01"]
    { <> intFromDigitChar(ch) }
/};

# Parses a single decimal digit (or spacer), returning its value.
def tokDecDigit = {/
    ch = ["_" "0".."9"]
    { <> intFromDigitChar(ch) }
/};

# Parses a single hexadecimal digit (or spacer), returning its value.
def tokHexDigit = {/
    ch = ["_" "0".."9" "a".."f" "A".."F"]
    { <> intFromDigitChar(ch) }
/};

# Parses an integer literal.
def implInt = {/
    baseSign = (
        "0x" ("-" { <> -16 } | { <> 16 })
    |
        "0b" ("-" { <> -2 } | { <> 2 })
    |
        { <> 10 }
    )

    digits = (
        { <> eq(baseSign, 10) }
        tokDecDigit+
    |
        { <> eq(iabs(baseSign), 16) }
        tokHexDigit+
    |
        { <> eq(iabs(baseSign), 2) }
        tokBinDigit+
    )

    { <> @[int: intFromDigitList(baseSign, digits)] }
/};
tokInt(implInt);

# Parses a single hexadecimal character. This is called during `\x...;`
# parsing.
def tokHexChar = {/
    digits = tokHexDigit+
    { <> charFromInt(intFromDigitList(16, digits)) }
/};

# Parses a single entity-named character. This is called during `\&...;`
# parsing.
def tokNamedChar = {/
    chars = ["a".."z" "A".."Z" "0".."9" "."]+
    {
        def name = stringFromTokenList(chars);
        <> mapGet(ENTITY_MAP, name)
    }
|
    "#x"
    digits = tokHexDigit+
    { <> charFromInt(intFromDigitList(16, digits)) }
|
    "#"
    digits = tokDecDigit+
    { <> charFromInt(intFromDigitList(10, digits)) }
/};

# Parses the inner portion of a string interpolation expression. This
# captures a list of inner tokens, ending when parentheses and braces are
# balanced out (and ignoring other could-be-matched delimiter tokens).
def tokStringInterpolation = forwardFunction();
def implStringInterpolation = {/
    &["({"]
    open = tokToken
    expectClose = { <> mapGet(OPEN_CLOSE_MAP, typeOf(open)) }

    tokens = (
        tokOptWhitespace
        (
            !["(){}"]
            !"/}"
            t = tokToken
            { <> [t] }
        |
            tokStringInterpolation
        )
    )*

    tokOptWhitespace
    close = tokToken

    {
        <> ifIs { <> eq(typeOf(close), expectClose) }
            { <> [open, listCat(tokens*)*, close] }
    }
/};
tokStringInterpolation(implStringInterpolation);

# Additional `stringPart` definitions for *Layer 2*.
def implStringPart2 = {/
    "\\"
    (
        "/"  { <> "" }
    |
        # This is the rule that ignores an escaped newline, followed by
        # any number of spaces.
        "\n"
        " "*
        { <> "" }
    |
        "x"
        (
            first = tokHexChar
            more = ("," tokHexChar)*
            ";"
            { <> stringCat(first, more*) }
        |
            { <> @[error: "Invalid hexadecimal character literal."] }
        )
    |
        "&"
        (
            first = tokNamedChar
            more = ("," tokNamedChar)*
            ";"
            { <> stringCat(first, more*) }
        |
            { <> @[error: "Invalid named character literal."] }
        )
    |
        format = (
            "%"
            chars = ["0".."9" "a".."z" "A".."Z" "+-."]*
            { <> [format: stringFromTokenList(chars)] }
        |
            { <> [:] }
        )

        &["({"]

        (
            tokens = tokStringInterpolation
            { <> [format*, tokens: tokens] }
        |
            { <> @[error: "Unbalanced string interpolation."] }
        )
    |
        { <> @[error: "Invalid character literal escape sequence."] }
    )
/};
tokStringPart2(implStringPart2);


#
# Exported Definitions
#

# Documented in Samizdat Layer 1 spec.
fn sam2Tokenize(programText) {
    <> pegApply(tokFile, programText)
};

<> [
    sam2Tokenize: sam2Tokenize
]

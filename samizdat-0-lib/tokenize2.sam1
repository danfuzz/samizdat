# Copyright 2013 the Samizdat Authors (Dan Bornstein et alia).
# Licensed AS IS and WITHOUT WARRANTY under the Apache License,
# Version 2.0. Details: <http://www.apache.org/licenses/LICENSE-2.0>

#
# Samizdat Layer 2 Tokenizer
#

#
# Helper definitions
#

# These are all the int digits, as a map from strings to digit values. This
# includes hex digits as well, in both lower and upper case. Finally, this
# includes a mapping of `"_"` to `-1` for the implementation of the
# "digit space" syntax.
def INT_CHARS = [
    "0": 0, "1": 1, "2": 2, "3": 3, "4": 4,
    "5": 5, "6": 6, "7": 7, "8": 8, "9": 9,
    "a": 10, "b": 11, "c": 12, "d": 13, "e": 14, "f": 15,
    "A": 10, "B": 11, "C": 12, "D": 13, "E": 14, "F": 15,
    "_": -1
];

# Helper to convert digit characters to digit values.
fn intFromDigitChar(ch) { <> mapGet(INT_CHARS, tokenType(ch)) };

# Map of all the keywords, from their string name to valueless tokens. These
# are (to a first approximation) operators whose spellings match the
# tokenization syntax of identifiers.
def KEYWORDS = listReduce([:],
    [
        "break", "collect", "continue", "def", "default", "do", "else",
        "export", "fn", "for", "if", "import", "in", "module", "reduce",
        "return", "switch", "var", "while"
    ])
    { result, ., name :: <> [result*, (name): @[(name)]] };

# Converts a list of digit values into an int, given the base. If `baseSign`
# is negative, this indicates that the result should be negative.
fn intFromDigitList(baseSign, digits) {
    def base = iabs(baseSign);
    def value = listReduce(0, digits) { result, ., digit ::
        <> ifIs { <> eq(digit, -1) }
            { <> result }
            { <> iadd(imul(result, base), digit) }
    };

    <> imul(value, isign(baseSign))
};


#
# Grammar rules
#

# Forward declaration of `tokToken`, for use in the interpolated string
# rule.
def tokToken = forwardFunction();

# Parses a `/* ... */` comment, with nesting.
def tokMultilineComment = forwardFunction();
def implMultilineComment = {/
    "/*"

    (
        tokMultilineComment
    |
        [! "*"]
    |
        "*" [! "/"]
    )*

    "*/"
/};
tokMultilineComment(implMultilineComment);

# Parses whitespace and comments. **Note:** The yielded result is always
# ignored.
def tokWhitespace = {/
    [" " "\n"]
|
    "#" [! "\n"]* "\n"
|
    tokMultilineComment
/};

# Parses punctuation and operators. The lookahead is done to avoid bothering
# with the choice expression unless we have a definite match.
def tokPunctuation = {/
    &["\\-/%&|^!@:,.=+?;*<>{}()[]"]
    (
        "\\==" | "\\!=" | "\\<=" | "\\>=" | "\\<" | "\\>"
    |
        "&&&" | "|||" | "^^^" | "!!!" | "<<<" | ">>>" | "..!" | "..+"
    |
        "@@" | "::" | "<>" | "()" | "&&" | "||" | ".." | "{/" | "/}" |
        "==" | "!=" | "<=" | ">="
    |
        # Note: We check this error condition here instead of in the
        # comment parsing code, because comments get parsed as whitespace,
        # which gets ignored. Rather than changing to not-quite-ignore
        # whitespace -- which would be messy -- we instead notice `/*` here
        # where we're parsing other punctuation. This is clean but a little
        # non-obvious, hence this comment.
        "/*" .*
        { <> @[error: "Unterminated comment."] }
    |
        # Single-character punctuation / operator.
        .
    )
/};

# Parses a single binary digit (or spacer), returning its value.
def tokBinDigit = {/
    ch = ["_01"]
    { <> intFromDigitChar(ch) }
/};

# Parses a single decimal digit (or spacer), returning its value.
def tokDecDigit = {/
    ch = ["_" "0".."9"]
    { <> intFromDigitChar(ch) }
/};

# Parses a single hexadecimal digit (or spacer), returning its value.
def tokHexDigit = {/
    ch = ["_" "0".."9" "a".."f" "A".."F"]
    { <> intFromDigitChar(ch) }
/};

# Parses an integer literal.
def tokInt = {/
    baseSign = (
        "0x" ("-" { <> -16 } | { <> 16 })
    |
        "0b" ("-" { <> -2 } | { <> 2 })
    |
        { <> 10 }
    )

    digits = (
        { <> eq(baseSign, 10) }
        tokDecDigit+
    |
        { <> eq(iabs(baseSign), 16) }
        tokHexDigit+
    |
        { <> eq(iabs(baseSign), 2) }
        tokBinDigit+
    )

    { <> @[int: intFromDigitList(baseSign, digits)] }
/};

# Parses a single hexadecimal character. This is called during `\x...;`
# parsing.
def tokHexChar = {/
    digits = tokHexDigit+
    { <> charFromInt(intFromDigitList(16, digits)) }
/};

# Parses a single entity-named character. This is called during `\&...;`
# parsing.
def tokNamedChar = {/
    chars = ["a".."z" "A".."Z" "0".."9" "."]+
    {
        def name = stringFromTokenList(chars);
        <> mapGet(ENTITY_MAP, name)
    }
/};

# Map from opening tokens to corresponding closers. Only includes mappings
# necessary to parse string interpolations.
def OPEN_CLOSE_MAP = [
    "(":  ")",
    "{":  "}",
    "{/": "/}"
];

# Parses the inner portion of a string interpolation expression. This
# captures a list of inner tokens, ending when parentheses and braces are
# balanced out (and ignoring other could-be-matched delimiter tokens).
def tokStringInterpolation = forwardFunction();
def implStringInterpolation = {/
    &["({"]
    open = tokToken
    expectClose = { <> mapGet(OPEN_CLOSE_MAP, tokenType(open)) }

    tokens = (
        tokWhitespace*
        (
            !["(){}"]
            !"/}"
            t = tokToken
            { <> [t] }
        |
            "()" # So as not to turn it into two separate tokens.
        |
            tokStringInterpolation
        )
    )*

    tokWhitespace*
    close = tokToken

    {
        <> ifIs { <> eq(tokenType(close), expectClose) }
            { <> [open, listAdd(tokens*)*, close] }
    }
/};
tokStringInterpolation(implStringInterpolation);

# Parses a single character, inside a quoted string.
def tokStringChar = {/
    (
        ch = [! "\\" "\"" "\n"]
        { <> tokenType(ch) }
    )
|
    # This is the rule that ignores spaces after newlines.
    "\n"
    " "*
    { <> "\n" }
|
    (
        "\\"
        (
            "\\" { <> "\\" } |
            "\"" { <> "\"" } |
            "n"  { <> "\n" } |
            "r"  { <> "\r" } |
            "t"  { <> "\t" } |
            "0"  { <> "\0" }
        |
            "/"  { <> "" }
        |
            # This is the rule that ignores the newline after end-of-line `\`.
            " "* "\n" " "*
            { <> "" }
        |
            "x"
            (
                first = tokHexChar
                more = ("," tokHexChar)*
                ";"
                { <> stringAdd(first, more*) }
            |
                { <> @[error: "Invalid hexadecimal character literal."] }
            )
        |
            "&"
            (
                first = tokNamedChar
                more = ("," tokNamedChar)*
                ";"
                { <> stringAdd(first, more*) }
            |
                { <> @[error: "Invalid named character literal."] }
            )
        |
            format = (
                "%"
                chars = ["0".."9" "a".."z" "A".."Z" "+-."]*
                { <> [format: stringFromTokenList(chars)] }
            |
                { <> [:] }
            )

            &["({"]

            (
                tokens = tokStringInterpolation
                { <> [format*, tokens: tokens] }
            |
                { <> @[error: "Unbalanced string interpolation."] }
            )
        |
            { <> @[error: "Invalid character literal escape sequence."] }
        )
    )
/};

# Parses a quoted string. This can yield either a `@string` token or
# an `@interpolatedString` token (or an `@error` token).
def tokString = {/
    "\""
    chars = tokStringChar*

    (
        "\""
        { <out> ::
            # Handle the empty string as a special case, to make the
            # reduce below be more straightforward.
            ifIs { <> eq(chars, []) }
                { <out> @[string: ""] };

            # Coalesce adjacent characters in `chars`. The result is an
            # `elems` list consisting of multi-character strings,
            # interpolation maps, and errors.
            def elems = listReduce([listFirst(chars)], listButFirst(chars))
                { result, ., ch ::
                    def last = listLast(result);
                    <> ifIs { <> and { <> isString(last) } { <> isString(ch) } }
                        { <> [listButLast(result)*, stringAdd(last, ch)] }
                        { <> [result*, ch] }
                };

            # Check for a simple result.
            ifIs { <> eq(lowSize(elems), 1) }
                {
                    def first = listFirst(elems);
                    ifIs { <> isString(first) }
                        {
                            # There's only one element, and it's a string. So,
                            # we have a simple non-interpolated string, and no
                            # error. Return it.
                            <out> @[string: first]
                        }
                };

            # At this point, we have an interpolation or an error. Check
            # for, and bail if we discover, the latter. The `isToken` test
            # works, because errors are the only tokens (per se) yielded
            # out of `tokStringChar`.
            listForEach(elems)
                { ., elem :: ifIs { <> isToken(elem) } { <out> elem } };

            <> @[interpolatedString: elems]
        }
    |
        { <> @[error: "Unterminated string literal."] }
    )
/};

# Parses an identifier (in the usual form). This also parses keywords.
def tokIdentifier = {/
    first = ["_" "a".."z" "A".."Z"]
    rest = ["_" "a".."z" "A".."Z" "0".."9"]*
    {
        def string = stringFromTokenList([first, rest*]);
        <> ifValue { <> mapGet(KEYWORDS, string) }
            { keyword :: <> keyword }
            { <> @[identifier: string] }
    }
/};

# Parses the quoted-string identifier form.
def tokQuotedIdentifier = {/
    "\\"
    s = tokString
    { <> @[identifier: tokenValue(s)] }
/};

# "Parses" an unrecognized character. This also consumes any further characters
# on the same line, in an attempt to resynch the input.
def tokError = {/
    badCh = .
    [! "\n"]*
    {
        def msg = stringAdd("Unrecognized character: ", tokenType(badCh));
        <> @[error: msg]
    }
/};

# Parses an arbitrary token or error.
def implToken = {/
    tokString | tokIdentifier | tokQuotedIdentifier
|
    # This needs to be listed after the quoted identifier rule, to
    # prevent `\"...` from being treated as a `\` token followed by
    # a string.
    tokPunctuation
|
    # This needs to be listed after the identifier rule, to prevent
    # an identifier-initial `_` from triggering this rule.
    tokInt
|
    tokError
/};
tokToken(implToken);

# Parses a file of tokens, yielding a list of them.
def tokFile = {/
    tokens=(tokWhitespace* tokToken)* tokWhitespace*
    { <> tokens }
/};


#
# Exported functions
#

# Documented in Samizdat Layer 1 spec.
fn sam2Tokenize(programText) {
    <> pegApply(tokFile, programText)
};

<> [
    sam2Tokenize: sam2Tokenize
]

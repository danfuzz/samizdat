# Copyright 2013 the Samizdat Authors (Dan Bornstein et alia).
# Licensed AS IS and WITHOUT WARRANTY under the Apache License,
# Version 2.0. Details: <http://www.apache.org/licenses/LICENSE-2.0>

#
# Samizdat Layer 2 Tokenizer
#

def Box = moduleGet({name: ["core", "Box"]});
def FunctionForwarder = moduleGet({name: ["core", "FunctionForwarder"]});
def ParseForwarder = moduleGet({name: ["core", "ParseForwarder"]});
def Peg = moduleGet({name: ["core", "Peg"]});


# The standard entity map.
def ENTITY_MAP = moduleGet({name: ["core", "EntityMap"]})::ENTITY_MAP;

# Forward declarations for Layer 2 definitions.
def tokMultilineComment = ParseForwarder::make();
def tokStringPart2 = ParseForwarder::make();


#
# This section is meant to mirror the code in the spec for tokenization as
# closely as possible, comments and all. The spec is a self-description of
# *Layer 1* parsing, and the comments should be taken in light of that.
#

# Map of all the keywords, from their string name to valueless tokens. These
# are (to a first approximation) operators whose spellings match the
# tokenization syntax of identifiers.
def KEYWORDS = Generator::collectAsMap(
    Generator::makeFilterGenerator([
        "def", "fn", "return",
        # *Layer 2* defines additional keywords here.
        "break", "continue", "default", "do", "else", "export",
        "for", "if", "import", "in", "module", "reduce",
        "switch", "var", "while"])
        { name <> {(name): @[(name)]} });

# These are all the int digits, as a map from strings to digit values. This
# includes hex digits as well, in both lower and upper case. Finally, this
# includes a mapping of `"_"` to `-1` for the implementation of the
# "digit space" syntax.
#
# **Note:** Only the decimal digits matter in *Layer 0* and *Layer 1*.
def INT_CHARS = {
    "0": 0, "1": 1, "2": 2, "3": 3, "4": 4,
    "5": 5, "6": 6, "7": 7, "8": 8, "9": 9,
    "a": 10, "b": 11, "c": 12, "d": 13, "e": 14, "f": 15,
    "A": 10, "B": 11, "C": 12, "D": 13, "E": 14, "F": 15,
    "_": -1
};

# Given a decimal digit, returns the digit value.
fn intFromDigitChar(ch) {
    <> get(INT_CHARS, typeOf(ch))
};

# Processes a list of `stringPart` elements, yielding a literal `string`
# value. In *Layer 2* (and higher) this can also yield an
# `interpolatedString` or an `error`.
def processStringParts = FunctionForwarder::make();

# Parses a non-zero amount of whitespace and comments.
# **Note:** The yielded result is always ignored.
def tokWhitespace = {/
    # The lookahead here is to avoid the bulk of this rule if there's
    # no chance we're actually looking at whitespace. The final
    # lookahead character is only useful as of *Layer 2*.
    &["# \n" "/"]

    (
        [" " "\n"]+
    |
        "#"
        (
            (["#! "] [! "\n"]*)
        |
            "\n"
        |
            !.
        )
    |
        # Introduced in *Layer 2*.
        &"/*" # Avoid calling the rule unless we know it will match.
        tokMultilineComment
    )+
/};

# Parses punctuation and operators.
#
# **Note:** This rule is expanded significantly in *Layer 2*.
def tokPunctuation = {/
    # The lookahead here is done to avoid bothering with the choice
    # expression, except when we have a definite match. The second
    # string in the lookahead calls out the characters that are only
    # needed in *Layer 1*. Yet more characters are included in *Layer 2*.
    &["&@:.,=-+?;*<>{}()[]" "/|!" "\\%^"]

    (
        # Introduced in *Layer 2*.
        "\\==" | "\\!=" | "\\<=" | "\\>=" | "\\<" | "\\>"
    |
        # Introduced in *Layer 2*.
        "&&&" | "|||" | "^^^" | "!!!" | "<<<" | ">>>" | "..!" | "..+"
    |
        # Introduced in *Layer 2*.
        "==" | "!=" | "<=" | ">=" | "??" | "**" | ":=" | "//" | "%%"
    |
        "->" | "::" | ".." | "<>"
    |
        # These are only needed in *Layer 1*.
        "{/" | "/}"
    |
        # Note: We check this error condition here instead of in the
        # comment parsing code, because comments get parsed as whitespace,
        # which gets ignored. Rather than changing to not-quite-ignore
        # whitespace -- which would be messy -- we instead notice `/*` here
        # where we're parsing other punctuation. This is clean but a little
        # non-obvious, hence this comment.
        "/*" .*
        { <> @[error: "Unterminated comment."] }
    |
        # Single-character punctuation / operator.
        .
    )
/};

# Parses an integer literal.
#
# **Note:** This rule is rewritten in *Layer 2*.
def tokInt = ParseForwarder::make();

# Parses a run of regular characters or an escape / special sequence,
# inside a quoted string.
#
# **Note:** Additional rules for string character parsing are defined
# in *Layer 2*.
def tokStringPart = {/
    (
        chars = [! "\\" "\"" "\n"]+
        { <> Peg::stringFromTokenList(chars) }
    )
|
    # This is the rule that ignores spaces after newlines.
    "\n"
    " "*
    { <> "\n" }
|
    "\\"
    (
        "\\" { <> "\\" } |
        "\"" { <> "\"" } |
        "n"  { <> "\n" } |
        "r"  { <> "\r" } |
        "t"  { <> "\t" } |
        "0"  { <> "\0" }
    )
|
    # Introduced in *Layer 2*.
    tokStringPart2
/};

# Parses a quoted string.
def tokString = {/
    "\""
    parts = tokStringPart*

    (
        "\""
        { <> processStringParts(parts) }
    |
        { <> @[error: "Unterminated string literal."] }
    )
/};

# Parses an identifier (in the usual form). This also parses keywords.
def tokIdentifier = {/
    one = ["_" "a".."z" "A".."Z"]
    rest = ["_" "a".."z" "A".."Z" "0".."9"]*

    {
        def string = Peg::stringFromTokenList([one, rest*]);
        <> ifValueOr { <> get(KEYWORDS, string) }
            { <> @[identifier: string] }
    }
/};

# Parses the quoted-string identifier form.
def tokQuotedIdentifier = {/
    "\\"
    s = tokString

    { <> @[identifier: dataOf(s)] }
/};

# "Parses" an unrecognized character. This also consumes any further characters
# on the same line, in an attempt to resynch the input.
def tokError = {/
    badCh = .
    [! "\n"]*

    {
        def msg = cat("Unrecognized character: ", typeOf(badCh));
        <> @[error: msg]
    }
/};

# Parses an arbitrary token or error.
def tokToken = {/
    tokString | tokIdentifier | tokQuotedIdentifier
|
    # This needs to be listed after the quoted identifier rule, to
    # prevent `\"...` from being treated as a `\` token followed by
    # a string.
    tokPunctuation
|
    # This needs to be listed after the identifier rule, to prevent
    # an identifier-initial `_` from triggering this rule. (This is
    # only significant in *Layer 2* and higher.)
    tokInt
|
    tokError
/};

# Parses a file of tokens, yielding a list of them.
def tokFile = {/
    tokens = (tokWhitespace? tokToken)*
    tokWhitespace?

    { <> tokens }
/};


#
# *Layer 2* definitions
#
# These are all specific to parsing *Layer 2* (and higher).
#

# Map from opening tokens to corresponding closers. Only includes mappings
# necessary to parse string interpolations.
def OPEN_CLOSE_MAP = {
    "(":  ")",
    "{":  "}",
    "{/": "/}"
};

# Converts a list of digit values into an int, given the base.
fn intFromDigitList(base, digits) {
    <> Generator::doReduce1(digits, 0) { digit, result ->
        <> ifIs { <> eq(digit, -1) }
            { <> result }
            { <> add(mul(result, base), digit) }
    };
};

fn implProcessStringParts(parts) {
    # Handle the empty string as a special case, to make the
    # reduce below be more straightforward.
    ifIs { <> eq(parts, []) }
        { return @[string: ""] };

    # If there is an error part (represented as an `@error` token), return
    # it.
    Generator::filterPump(parts)
        { part ->
            <> ifIs { <> eq(typeOf(part), "error") }
                { return part }
        };

    # Coalesce adjacent literal strings in `parts`. The result is an
    # `elems` list consisting of multi-character strings and
    # interpolation maps.
    def elems = Generator::doReduce1(butFirst(parts), [first(parts)])
        { part, result ->
            <> ifValue { <> [&isString(last(result)), &isString(part)] }
                { newLast <> [butLast(result)*, cat(newLast*)] }
                { <> [result*, part] };
        };

    # Check for a simple result.
    ifIs { <> eq(sizeOf(elems), 1) }
        {
            ifValue { <> isString(elems*) }
                { elem ->
                    # There's only one element, and it's a string. So,
                    # we have a simple non-interpolated string. Return it.
                    return @[string: elem]
                }
        };

    # At this point, we have a valid interpolation.
    <> @[interpolatedString: elems]
};
Box::store(processStringParts, implProcessStringParts);

# Parses a `/* ... */` comment, with nesting.
def implMultilineComment = {/
    "/*"

    (
        tokMultilineComment
    |
        [! "*"]
    |
        "*" !"/"
    )*

    "*/"
/};
Box::store(tokMultilineComment, implMultilineComment);

# Parses a single binary digit (or spacer), returning its value.
def tokBinDigit = {/
    ch = ["_01"]
    { <> intFromDigitChar(ch) }
/};

# Parses a single decimal digit (or spacer), returning its value.
def tokDecDigit = {/
    ch = ["_" "0".."9"]
    { <> intFromDigitChar(ch) }
/};

# Parses a single hexadecimal digit (or spacer), returning its value.
def tokHexDigit = {/
    ch = ["_" "0".."9" "a".."f" "A".."F"]
    { <> intFromDigitChar(ch) }
/};

# Parses an integer literal.
def implInt = {/
    base = (
        "0x" { <> 16 }
    |
        "0b" { <> 2 }
    |
        { <> 10 }
    )

    digits = (
        { <> eq(base, 10) }
        tokDecDigit+
    |
        { <> eq(base, 16) }
        tokHexDigit+
    |
        { <> eq(base, 2) }
        tokBinDigit+
    )

    { <> @[int: intFromDigitList(base, digits)] }
/};
Box::store(tokInt, implInt);

# Parses a single hexadecimal character. This is called during `\x...;`
# parsing.
def tokHexChar = {/
    digits = tokHexDigit+
    { <> toString(intFromDigitList(16, digits)) }
/};

# Parses a single entity-named character. This is called during `\&...;`
# parsing.
def tokNamedChar = {/
    chars = ["a".."z" "A".."Z" "0".."9" "."]+
    {
        def name = Peg::stringFromTokenList(chars);
        <> get(ENTITY_MAP, name)
    }
|
    "#x"
    digits = tokHexDigit+
    { <> toString(intFromDigitList(16, digits)) }
|
    "#"
    digits = tokDecDigit+
    { <> toString(intFromDigitList(10, digits)) }
/};

# Parses the inner portion of a string interpolation expression. This
# captures a list of inner tokens, ending when parentheses and braces are
# balanced out (and ignoring other could-be-matched delimiter tokens).
def tokStringInterpolation = ParseForwarder::make();
def implStringInterpolation = {/
    &["({"]
    open = tokToken
    expectClose = { <> get(OPEN_CLOSE_MAP, typeOf(open)) }

    tokens = (
        tokWhitespace?
        (
            !["(){}"]
            !"/}"
            t = tokToken
            { <> [t] }
        |
            tokStringInterpolation
        )
    )*

    tokWhitespace?
    close = tokToken

    {
        <> ifIs { <> eq(typeOf(close), expectClose) }
            { <> [open, cat([], tokens*)*, close] }
    }
/};
Box::store(tokStringInterpolation, implStringInterpolation);

# Additional `stringPart` definitions for *Layer 2*.
def implStringPart2 = {/
    "\\"
    (
        "/"  { <> "" }
    |
        # This is the rule that ignores an escaped newline, followed by
        # any number of spaces.
        "\n"
        " "*
        { <> "" }
    |
        "x"
        (
            one = tokHexChar
            rest = ("," tokHexChar)*
            ";"
            { <> cat(one, rest*) }
        |
            { <> @[error: "Invalid hexadecimal character literal."] }
        )
    |
        "&"
        (
            one = tokNamedChar
            rest = ("," tokNamedChar)*
            ";"
            { <> cat(one, rest*) }
        |
            { <> @[error: "Invalid named character literal."] }
        )
    |
        format = (
            "%"
            chars = ["0".."9" "a".."z" "A".."Z" "+-."]*
            { <> {format: Peg::stringFromTokenList(chars)} }
        |
            { <> {} }
        )

        &["({"]

        (
            tokens = tokStringInterpolation
            { <> {format*, tokens: tokens} }
        |
            { <> @[error: "Unbalanced string interpolation."] }
        )
    |
        { <> @[error: "Invalid character literal escape sequence."] }
    )
/};
Box::store(tokStringPart2, implStringPart2);


#
# Exported Definitions
#

# Documented in Samizdat Layer 1 spec.
fn samTokenize(programText) {
    <> Peg::apply(tokFile, programText)
};

<> {
    samTokenize: samTokenize
}

# Copyright 2013 the Samizdat Authors (Dan Bornstein et alia).
# Licensed AS IS and WITHOUT WARRANTY under the Apache License,
# Version 2.0. Details: <http://www.apache.org/licenses/LICENSE-2.0>

#
# Samizdat Layer 2 Tokenizer
#


# Forward declarations for Layer 2 definitions.
def tokMultilineComment = forwardFunction();
def tokStringPart2 = forwardFunction();


#
# This section is meant to mirror the code in the spec for tokenization as
# closely as possible, comments and all. The spec is a self-description of
# *Layer 1* parsing, and the comments should be taken in light of that.
#

# Map of all the keywords, from their string name to valueless tokens. These
# are (to a first approximation) operators whose spellings match the
# tokenization syntax of identifiers.
def KEYWORDS = mapFromGenerator(
    filterGenerator([
        "def", "fn", "return",
        # *Layer 2* defines additional keywords here.
        "break", "collect", "continue", "default", "do", "else",
        "export", "for", "if", "import", "in", "module", "reduce",
        "switch", "var", "while"])
        { name :: <> [(name): @[(name)]] });

# These are all the int digits, as a map from strings to digit values. This
# includes hex digits as well, in both lower and upper case. Finally, this
# includes a mapping of `"_"` to `-1` for the implementation of the
# "digit space" syntax.
#
# **Note:** Only the decimal digits matter in *Layer 0* and *Layer 1*.
def INT_CHARS = [
    "0": 0, "1": 1, "2": 2, "3": 3, "4": 4,
    "5": 5, "6": 6, "7": 7, "8": 8, "9": 9,
    "a": 10, "b": 11, "c": 12, "d": 13, "e": 14, "f": 15,
    "A": 10, "B": 11, "C": 12, "D": 13, "E": 14, "F": 15,
    "_": -1
];

# Given a decimal digit, returns the digit value.
fn intFromDigitChar(ch) { <> mapGet(INT_CHARS, tokenType(ch)) };

# Parses whitespace and comments. **Note:** The yielded result is always
# ignored.
def tokWhitespace = {/
    [" " "\n"]
|
    "#" [! "\n"]* "\n"
|
    # Introduced in *Layer 2*.
    tokMultilineComment
/};

# Parses punctuation and operators. The lookahead is done to avoid bothering
# with the choice expression unless we have a definite match.
#
# **Note:** This rule is expanded significantly in *Layer 2*.
def tokPunctuation = {/
    # The lookahead here is done to avoid bothering with the choice
    # expression, except when we have a definite match. The second
    # string in the lookahead calls out the characters that are only
    # needed in *Layer 1*. Yet more characters are included in *Layer 2*.
    &["&@:.,=-+?;*<>{}()[]" "/|!" "\\%^"]

    (
        # Introduced in *Layer 2*.
        "\\==" | "\\!=" | "\\<=" | "\\>=" | "\\<" | "\\>"
    |
        # Introduced in *Layer 2*.
        "&&&" | "|||" | "^^^" | "!!!" | "<<<" | ">>>" | "..!" | "..+"
    |
        # Introduced in *Layer 2*.
        "==" | "!=" | "<=" | ">=" | "??" | "**"
    |
        "@@" | "::" | ".." | "<>"
    |
        # These are only needed in *Layer 1*.
        "{/" | "/}"
    |
        # Note: We check this error condition here instead of in the
        # comment parsing code, because comments get parsed as whitespace,
        # which gets ignored. Rather than changing to not-quite-ignore
        # whitespace -- which would be messy -- we instead notice `/*` here
        # where we're parsing other punctuation. This is clean but a little
        # non-obvious, hence this comment.
        "/*" .*
        { <> @[error: "Unterminated comment."] }
    |
        # Single-character punctuation / operator.
        .
    )
/};

# Parses an integer literal.
#
# **Note:** This rule is rewritten in *Layer 2*.
def tokInt = forwardFunction();

# Parses a run of regular characters or an escape / special sequence,
# inside a quoted string.
#
# **Note:** Additional rules for string character parsing are defined
# in *Layer 2*.
def tokStringPart = {/
    (
        chars = [! "\\" "\"" "\n"]+
        { <> stringFromTokenList(chars) }
    )
|
    # This is the rule that ignores spaces after newlines.
    "\n"
    " "*
    { <> "\n" }
|
    "\\"
    (
        "\\" { <> "\\" } |
        "\"" { <> "\"" } |
        "n"  { <> "\n" } |
        "r"  { <> "\r" } |
        "t"  { <> "\t" } |
        "0"  { <> "\0" }
    )
|
    # Introduced in *Layer 2*.
    tokStringPart2
/};

# Parses a quoted string.
#
# **Note:** In Layer 2, this rule is expanded to yield either a `@string`
# token or an `@interpolatedString` token (or an `@error` token).
def tokString = {/
    "\""
    parts = tokStringPart*

    (
        "\""
    |
        { <> @[error: "Unterminated string literal."] }
    )

    { <out> ::
        # Handle the empty string as a special case, to make the
        # reduce below be more straightforward.
        ifIs { <> eq(parts, []) }
            { <out> @[string: ""] };

        # Coalesce adjacent literal strings in `parts`. The result is an
        # `elems` list consisting of multi-character strings and
        # interpolation maps. This loop also takes care of propagating
        # any found error as the overall result of this rule.
        def elems = doReduce1(listButFirst(parts), [listFirst(parts)])
            { <next> part, result ::
                ifIs { <> isToken(part) }
                    {
                        # Errors come in the form of tokens. (Nothing else
                        # will be a token.)
                        <out> part
                    };
                ifIs { <> isString(part) }
                    {
                        def last = listLast(result);
                        ifIs { <> isString(last) }
                            {
                                # Coalesce adjacent literal string parts.
                                def newLast = stringAdd(last, part);
                                <next> [listButLast(result)*, newLast]
                            }
                    };
                <> [result*, part]
            };

        # Check for a simple result.
        ifIs { <> eq(lowSize(elems), 1) }
            {
                def first = listFirst(elems);
                ifIs { <> isString(first) }
                    {
                        # There's only one element, and it's a string. So,
                        # we have a simple non-interpolated string, and no
                        # error. Return it.
                        <out> @[string: first]
                    }
            };

        # At this point, we have a interpolation.
        <> @[interpolatedString: elems]
    }
/};

# Parses an identifier (in the usual form). This also parses keywords.
def tokIdentifier = {/
    first = ["_" "a".."z" "A".."Z"]
    rest = ["_" "a".."z" "A".."Z" "0".."9"]*

    {
        def string = stringFromTokenList([first, rest*]);
        <> ifValueOr { <> mapGet(KEYWORDS, string) }
            { <> @[identifier: string] }
    }
/};

# Parses the quoted-string identifier form.
def tokQuotedIdentifier = {/
    "\\"
    s = tokString

    { <> @[identifier: tokenValue(s)] }
/};

# "Parses" an unrecognized character. This also consumes any further characters
# on the same line, in an attempt to resynch the input.
def tokError = {/
    badCh = .
    [! "\n"]*

    {
        def msg = stringAdd("Unrecognized character: ", tokenType(badCh));
        <> @[error: msg]
    }
/};

# Parses an arbitrary token or error.
def tokToken = {/
    tokString | tokIdentifier | tokQuotedIdentifier
|
    # This needs to be listed after the quoted identifier rule, to
    # prevent `\"...` from being treated as a `\` token followed by
    # a string.
    tokPunctuation
|
    # This needs to be listed after the identifier rule, to prevent
    # an identifier-initial `_` from triggering this rule. (This is
    # only significant in *Layer 2* and higher.)
    tokInt
|
    tokError
/};

# Parses a file of tokens, yielding a list of them.
def tokFile = {/
    tokens = (tokWhitespace* tokToken)*
    tokWhitespace*

    { <> tokens }
/};


#
# *Layer 2* definitions
#
# These are all specific to parsing *Layer 2* (and higher).
#

# Map from opening tokens to corresponding closers. Only includes mappings
# necessary to parse string interpolations.
def OPEN_CLOSE_MAP = [
    "(":  ")",
    "{":  "}",
    "{/": "/}"
];

# Converts a list of digit values into an int, given the base. If `baseSign`
# is negative, this indicates that the result should be negative.
fn intFromDigitList(baseSign, digits) {
    def base = iabs(baseSign);
    def value = doReduce1(digits, 0) { digit, result ::
        <> ifIs { <> eq(digit, -1) }
            { <> result }
            { <> iadd(imul(result, base), digit) }
    };

    <> imul(value, isign(baseSign))
};

# Parses a `/* ... */` comment, with nesting.
def implMultilineComment = {/
    "/*"

    (
        tokMultilineComment
    |
        [! "*"]
    |
        "*" !"/"
    )*

    "*/"
/};
tokMultilineComment(implMultilineComment);

# Parses a single binary digit (or spacer), returning its value.
def tokBinDigit = {/
    ch = ["_01"]
    { <> intFromDigitChar(ch) }
/};

# Parses a single decimal digit (or spacer), returning its value.
def tokDecDigit = {/
    ch = ["_" "0".."9"]
    { <> intFromDigitChar(ch) }
/};

# Parses a single hexadecimal digit (or spacer), returning its value.
def tokHexDigit = {/
    ch = ["_" "0".."9" "a".."f" "A".."F"]
    { <> intFromDigitChar(ch) }
/};

# Parses an integer literal.
def implInt = {/
    baseSign = (
        "0x" ("-" { <> -16 } | { <> 16 })
    |
        "0b" ("-" { <> -2 } | { <> 2 })
    |
        { <> 10 }
    )

    digits = (
        { <> eq(baseSign, 10) }
        tokDecDigit+
    |
        { <> eq(iabs(baseSign), 16) }
        tokHexDigit+
    |
        { <> eq(iabs(baseSign), 2) }
        tokBinDigit+
    )

    { <> @[int: intFromDigitList(baseSign, digits)] }
/};
tokInt(implInt);

# Parses a single hexadecimal character. This is called during `\x...;`
# parsing.
def tokHexChar = {/
    digits = tokHexDigit+
    { <> charFromInt(intFromDigitList(16, digits)) }
/};

# Parses a single entity-named character. This is called during `\&...;`
# parsing.
def tokNamedChar = {/
    chars = ["a".."z" "A".."Z" "0".."9" "."]+
    {
        def name = stringFromTokenList(chars);
        <> mapGet(ENTITY_MAP, name)
    }
|
    "#x"
    digits = tokHexDigit+
    { <> charFromInt(intFromDigitList(16, digits)) }
|
    "#"
    digits = tokDecDigit+
    { <> charFromInt(intFromDigitList(10, digits)) }
/};

# Parses the inner portion of a string interpolation expression. This
# captures a list of inner tokens, ending when parentheses and braces are
# balanced out (and ignoring other could-be-matched delimiter tokens).
def tokStringInterpolation = forwardFunction();
def implStringInterpolation = {/
    &["({"]
    open = tokToken
    expectClose = { <> mapGet(OPEN_CLOSE_MAP, tokenType(open)) }

    tokens = (
        tokWhitespace*
        (
            !["(){}"]
            !"/}"
            t = tokToken
            { <> [t] }
        |
            tokStringInterpolation
        )
    )*

    tokWhitespace*
    close = tokToken

    {
        <> ifIs { <> eq(tokenType(close), expectClose) }
            { <> [open, listAdd(tokens*)*, close] }
    }
/};
tokStringInterpolation(implStringInterpolation);

# Additional `stringPart` definitions for *Layer 2*.
def implStringPart2 = {/
    "\\"
    (
        "/"  { <> "" }
    |
        # This is the rule that ignores the newline after end-of-line `\`.
        " "* "\n" " "*
        { <> "" }
    |
        "x"
        (
            first = tokHexChar
            more = ("," tokHexChar)*
            ";"
            { <> stringAdd(first, more*) }
        |
            { <> @[error: "Invalid hexadecimal character literal."] }
        )
    |
        "&"
        (
            first = tokNamedChar
            more = ("," tokNamedChar)*
            ";"
            { <> stringAdd(first, more*) }
        |
            { <> @[error: "Invalid named character literal."] }
        )
    |
        format = (
            "%"
            chars = ["0".."9" "a".."z" "A".."Z" "+-."]*
            { <> [format: stringFromTokenList(chars)] }
        |
            { <> [:] }
        )

        &["({"]

        (
            tokens = tokStringInterpolation
            { <> [format*, tokens: tokens] }
        |
            { <> @[error: "Unbalanced string interpolation."] }
        )
    |
        { <> @[error: "Invalid character literal escape sequence."] }
    )
/};
tokStringPart2(implStringPart2);


#
# Exported functions
#

# Documented in Samizdat Layer 1 spec.
fn sam2Tokenize(programText) {
    <> pegApply(tokFile, programText)
};

<> [
    sam2Tokenize: sam2Tokenize
]

## Copyright 2013-2014 the Samizdat Authors (Dan Bornstein et alia).
## Licensed AS IS and WITHOUT WARRANTY under the Apache License,
## Version 2.0. Details: <http://www.apache.org/licenses/LICENSE-2.0>

##
## Samizdat Layer 2 Tokenizer
##


## The standard entity map.
def ENTITY_MAP = moduleUse({name: ["core", "EntityMap"]})::ENTITY_MAP;

## Forward declarations for Layer 2 definitions.
def tokMultilineComment;
def tokStringPart2;


##
## This section is meant to mirror the code in the spec for tokenization as
## closely as possible, comments and all. The spec is a self-description of
## Layer 1 parsing, and the comments should be taken in light of that.
##

## Map of all the keywords, from their string name to valueless tokens. These
## are (to a first approximation) operators whose spellings match the
## tokenization syntax of identifiers.
def KEYWORDS = Generator::collectAsMap(
    Generator::makeFilterGenerator([
        "break", "continue", "def", "fn", "return", "var",
        ## Layer 2 defines additional keywords here.
        "by", "default", "do", "else", "export", "for", "if", "import",
        "in", "module", "switch", "while"])
        { name <> {(name): @(name)} });

## These are all the int digits, as a map from strings to digit values. This
## includes hex digits as well, in both lower and upper case. Finally, this
## includes a mapping of `"_"` to `-1` for the implementation of the
## "digit space" syntax.
##
## **Note:** Only the decimal digits matter in Layer 0 and Layer 1.
def INT_CHARS = {
    "0": 0, "1": 1, "2": 2, "3": 3, "4": 4,
    "5": 5, "6": 6, "7": 7, "8": 8, "9": 9,
    "a": 10, "b": 11, "c": 12, "d": 13, "e": 14, "f": 15,
    "A": 10, "B": 11, "C": 12, "D": 13, "E": 14, "F": 15,
    "_": -1
};

## Given a decimal digit, returns the digit value.
fn intFromDigitChar(ch) {
    <> get(INT_CHARS, typeNameOf(ch))
};

## Converts a list of digit values into an int, given the base.
fn intFromDigitList(base, digits) {
    var result = 0;

    Generator::filterPump(digits) { digit ->
        ifIs { <> perNe(digit, -1) }
            { result := Number::add(Number::mul(result, base), digit) }
    };

    <> result
};

## Parses a non-zero amount of whitespace and comments.
## **Note:** The yielded result is always ignored.
def tokWhitespace = {:
    ## The lookahead here is to avoid the bulk of this rule if there's
    ## no chance we're actually looking at whitespace.
    &["# \n"]

    (
        [" " "\n"]+
    |
        "#" ["#!"] [! "\n"]*
    |
        ## Introduced in Layer 2.
        &"#:" ## Avoid calling the rule unless we know it will match.
        %tokMultilineComment
    )+
:};

## Parses punctuation and operators.
##
## **Note:** This rule is expanded significantly in Layer 2.
def tokPunctuation = {:
    ## The lookahead here is done to avoid bothering with the choice
    ## expression, except when we have a definite match. The second
    ## string in the lookahead calls out the characters that are only
    ## needed in Layer 1. Yet more characters are included in Layer 2.
    &["@:.,=-+?;*<>{}()[]" "&|!%" "/\\^#"]

    (
        ## Introduced in Layer 2.
        "\\==" | "\\!=" | "\\<=" | "\\>=" | "\\<" | "\\>"
    |
        ## Introduced in Layer 2.
        "&&&" | "|||" | "^^^" | "!!!" | "<<<" | ">>>" | "..!"
    |
        ## Introduced in Layer 2.
        "==" | "!=" | "<=" | ">=" | "??" | "**" | "//" | "%%"
    |
        "->" | ":=" | "::" | ".." | "<>" | "@@"
    |
        ## These are introduced in Layer 1.
        "{:" | ":}"
    |
        ## Note: We check this error condition here instead of in the
        ## comment parsing code, because comments get parsed as whitespace,
        ## which gets ignored. Rather than changing to not-quite-ignore
        ## whitespace -- which would be messy -- we instead notice `#:` here
        ## where we're parsing other punctuation. This is clean but a little
        ## non-obvious, hence this comment.
        "#:" .*
        { <> @error("Unterminated comment.") }
    |
        ## Single-character punctuation / operator.
        .
    )
:};

## Parses an integer literal.
##
## **Note:** This rule is rewritten in Layer 2.
def tokInt;

## Parses a run of regular characters or an escape / special sequence,
## inside a quoted string.
##
## **Note:** Additional rules for string character parsing are defined
## in Layer 2.
def tokStringPart = {:
    (
        chars = [! "\\" "\"" "\n"]+
        { <> Peg::stringFromTokenList(chars) }
    )
|
    ## This is the rule that ignores spaces after newlines.
    "\n"
    " "*
    { <> "\n" }
|
    "\\"
    (
        "\\" { <> "\\" } |
        "\"" { <> "\"" } |
        "n"  { <> "\n" } |
        "r"  { <> "\r" } |
        "t"  { <> "\t" } |
        "0"  { <> "\0" }
    )
|
    ## Introduced in Layer 2.
    %tokStringPart2
:};

## Parses a quoted string.
def tokString = {:
    "\""
    parts = tokStringPart*

    (
        "\""
        { <> processStringParts(parts) }
    |
        { <> @error("Unterminated string literal.") }
    )
:};

## Parses an identifier (in the usual form). This also parses keywords.
def tokIdentifier = {:
    one = ["_" "a".."z" "A".."Z"]
    rest = ["_" "a".."z" "A".."Z" "0".."9"]*

    {
        def string = Peg::stringFromTokenList([one, rest*]);
        <> ifValueOr { <> get(KEYWORDS, string) }
            { <> @identifier(string) }
    }
:};

## Parses the quoted-string identifier form.
def tokQuotedIdentifier = {:
    "\\"
    s = tokString

    { <> @identifier(dataOf(s)) }
:};

## "Parses" an unrecognized character. This also consumes any further
## characters on the same line, in an attempt to resynch the input.
def tokError = {:
    badCh = .
    [! "\n"]*

    {
        def msg = cat("Unrecognized character: ", typeNameOf(badCh));
        <> @error(msg)
    }
:};

## Parses an arbitrary token or error.
def tokToken = {:
    tokString | tokIdentifier | tokQuotedIdentifier
|
    ## This needs to be listed after the quoted identifier rule, to
    ## prevent `\"...` from being treated as a `\` token followed by
    ## a string.
    tokPunctuation
|
    ## This needs to be listed after the identifier rule, to prevent
    ## an identifier-initial `_` from triggering this rule. (This is
    ## only significant in Layer 2 and higher.)
    %tokInt
|
    tokError
:};

## Parses a file of tokens, yielding a list of them.
def tokFile = {:
    tokens = (tokWhitespace? tokToken)*
    tokWhitespace?

    { <> tokens }
:};


##
## Layer 2 definitions
##
## These are all specific to parsing Layer 2 (and higher).
##

## Map from opening tokens to corresponding closers. Used when parsing
## string interpolations.
def OPEN_CLOSE_MAP = {
    @@"(":  @@")",
    @@"[":  @@"]",
    @@"{":  @@"}",
    @@"{:": @@":}"
};

## Processes a list of `stringPart` elements, yielding a literal `string`
## token, an `interpolatedString`, or an `error`.
fn processStringParts(parts) {
    ## Handle the empty string as a special case.
    ifIs { <> eq(parts, []) }
        { return @string("") };

    ## Coalesce adjacent literal strings in `parts`. The result is an
    ## `elems` list consisting of alternating multi-character strings and
    ## interpolation maps.
    var elems = [];
    Generator::filterPump(parts) { <next> part ->
        ## If there is an error part (represented as an `@error` token),
        ## return it.
        ifIs { <> hasType(part, @@error) }
            { return part };

        ifIs { <> eq(elems, []) }
            {
                ## First element. No possibility to coalesce, so special-case
                ## it.
                elems := [part];
                <next>
            };

        ifIs { <> isString(part) }
            {
                def lastElem = Sequence::nthFromEnd(elems, 0);
                ifIs { <> isString(lastElem) }
                    {
                        ## The last item in `elems` and the current `part`
                        ## are both strings. Combine them.
                        elems := [
                            Sequence::sliceExclusive(elems, 0)*,
                            cat(lastElem, part)
                        ];
                        <next>
                    }
            };

        elems := [elems*, part]
    };

    ## Check for a simple result.
    ifIs { <> eq(Collection::sizeOf(elems), 1) }
        {
            ifValue { <> isString(elems*) }
                { elem ->
                    ## There's only one element, and it's a string. So,
                    ## we have a simple non-interpolated string. Return it.
                    return @string(elem)
                }
        };

    ## At this point, we have a valid interpolation.
    <> @interpolatedString(elems)
};

## Parses a `#: ... :#` comment, with nesting.
tokMultilineComment := {:
    "#:"

    (
        %tokMultilineComment
    |
        [! ":"]
    |
        ":" !"#"
    )*

    ":#"
:};

## Parses a single binary digit (or spacer), returning its value.
def tokBinDigit = {:
    ch = ["_01"]
    { <> intFromDigitChar(ch) }
:};

## Parses a single decimal digit (or spacer), returning its value.
def tokDecDigit = {:
    ch = ["_" "0".."9"]
    { <> intFromDigitChar(ch) }
:};

## Parses a single hexadecimal digit (or spacer), returning its value.
def tokHexDigit = {:
    ch = ["_" "0".."9" "a".."f" "A".."F"]
    { <> intFromDigitChar(ch) }
:};

## Parses an integer literal.
tokInt := {:
    base = (
        "0x" { <> 16 }
    |
        "0b" { <> 2 }
    |
        { <> 10 }
    )

    digits = (
        { <> eq(base, 10) }
        tokDecDigit+
    |
        { <> eq(base, 16) }
        tokHexDigit+
    |
        { <> eq(base, 2) }
        tokBinDigit+
    )

    { <> @int(intFromDigitList(base, digits)) }
:};

## Parses a single hexadecimal character. This is called during `\x...;`
## parsing.
def tokHexChar = {:
    digits = tokHexDigit+
    { <> toString(intFromDigitList(16, digits)) }
:};

## Parses a single entity-named character. This is called during `\&...;`
## parsing.
def tokNamedChar = {:
    chars = ["a".."z" "A".."Z" "0".."9" "."]+
    {
        def name = Peg::stringFromTokenList(chars);
        <> get(ENTITY_MAP, name)
    }
|
    "#x"
    digits = tokHexDigit+
    { <> toString(intFromDigitList(16, digits)) }
|
    "#"
    digits = tokDecDigit+
    { <> toString(intFromDigitList(10, digits)) }
:};

## Parses the inner portion of a string interpolation expression. This
## captures a list of inner tokens, ending when parentheses and braces are
## balanced out (and ignoring other could-be-matched delimiter tokens).
def tokStringInterpolation;
tokStringInterpolation := {:
    &["({["]
    open = tokToken
    expectClose = { <> get(OPEN_CLOSE_MAP, typeOf(open)) }

    tokens = (
        tokWhitespace?
        (
            !["(){}[]"]
            !":}"
            t = tokToken
            { <> [t] }
        |
            %tokStringInterpolation
        )
    )*

    tokWhitespace?
    close = tokToken

    {
        <> ifIs { <> hasType(close, expectClose) }
            { <> [open, cat([], tokens*)*, close] }
    }
:};

## Additional `stringPart` definitions for Layer 2.
tokStringPart2 := {:
    "\\"
    (
        "/"  { <> "" }
    |
        ## This is the rule that ignores an escaped newline, followed by
        ## any number of spaces.
        "\n"
        " "*
        { <> "" }
    |
        "x"
        (
            one = tokHexChar
            rest = ("," tokHexChar)*
            ";"
            { <> cat(one, rest*) }
        |
            { <> @error("Invalid hexadecimal character literal.") }
        )
    |
        "&"
        (
            one = tokNamedChar
            rest = ("," tokNamedChar)*
            ";"
            { <> cat(one, rest*) }
        |
            { <> @error("Invalid named character literal.") }
        )
    |
        format = (
            "%"
            chars = ["0".."9" "a".."z" "A".."Z" "+-."]*
            { <> {format: Peg::stringFromTokenList(chars)} }
        |
            { <> {} }
        )

        &["({["]

        (
            tokens = tokStringInterpolation
            { <> {format*, tokens} }
        |
            { <> @error("Unbalanced string interpolation.") }
        )
    |
        { <> @error("Invalid character literal escape sequence.") }
    )
:};


##
## Exported Definitions
##

## Documented in spec.
fn tokenize(programText) {
    <> Peg::apply(tokFile, programText)
};

<> {
    tokenize
}

## Copyright 2013-2014 the Samizdat Authors (Dan Bornstein et alia).
## Licensed AS IS and WITHOUT WARRANTY under the Apache License,
## Version 2.0. Details: <http://www.apache.org/licenses/LICENSE-2.0>

##
## Samizdat Layer 2 Tokenizer
##

#= language core.Lang1

import core.EntityMap :: ENTITY_MAP;
import core.Generator;
import core.Peg;
import core.Sequence;
import proto.Number;


##
## This section is meant to mirror the code in the spec for tokenization as
## closely as possible, comments and all. The spec is a self-description of
## Layer 1 parsing, and the comments should be taken in light of that.
##

##
## Private Definitions
##

## Map of all the keywords, from their string name to valueless tokens. These
## are (to a first approximation) operators whose spellings match the
## tokenization syntax of identifiers.
def KEYWORDS = $Generator::collectAsMap(
    $Generator::makeFilterGenerator([
        "break", "continue", "def", "export", "fn", "import", "return",
        "var", "yield",
        ## Layer 2 defines additional keywords here.
        "by", "default", "do", "else", "for", "if", "in", "module", "switch",
        "while"])
        { name -> {(name): @(@@(name))} });

## These are all the int digits, as a map from strings to digit values. This
## includes hex digits as well, in both lower and upper case. Finally, this
## includes a mapping of `"_"` to `-1` for the implementation of the
## "digit space" syntax.
##
## **Note:** Only the decimal digits matter in Layer 0 and Layer 1.
def INT_CHARS = {
    "0": 0, "1": 1, "2": 2, "3": 3, "4": 4,
    "5": 5, "6": 6, "7": 7, "8": 8, "9": 9,
    "a": 10, "b": 11, "c": 12, "d": 13, "e": 14, "f": 15,
    "A": 10, "B": 11, "C": 12, "D": 13, "E": 14, "F": 15,
    "_": -1
};

## Given a decimal digit, returns the digit value.
fn intFromDigitChar(ch) {
    return get(INT_CHARS, get_typeName(ch))
};

## Converts a list of digit values into an int, given the base.
fn intFromDigitList(base, digits) {
    var result = 0;

    $Generator::filterPump(digits) { digit ->
        ifIs { perNe(digit, -1) }
            { result := $Number::add($Number::mul(result, base), digit) }
    };

    return result
};


##
## Layer 0 Rules
##
## This section consists of the definitions required to implement Layer 0,
## with comments indicating the "hooks" for higher layers.
##

## Forward declarations required for layer 2. These are all add-ons to
## layer 0 or 1 rules, used to expand the syntactic possibilities of the
## indicated base forms.
def tokInt2;
def tokMultilineComment;
def tokStringPart2;

## Parses any amount of whitespace, comments, and directives (including
## nothing at all). **Note:** The yielded result is always ignored.
def tokWhitespace = {:
    ## The lookahead here is to avoid the bulk of this rule if there's
    ## no chance we're actually looking at whitespace.
    &["# \n"]

    (
        [" \n"]+
    |
        "#" ["#!="] [! "\n"]*
    |
        ## Introduced in Layer 2.
        &"#:" ## Avoid calling the rule unless we know it will match.
        %tokMultilineComment
    )+
:};

## Parses punctuation and operators.
##
## **Note:** This rule is expanded significantly in Layer 2.
def tokPunctuation = {:
    ## The lookahead here is done to avoid bothering with the choice
    ## expression, except when we have a definite match. The second
    ## string in the lookahead calls out the characters that are only
    ## needed in Layer 1. Yet more characters are included in Layer 2.
    &["@:.,=-+?;*/<>{}()[]" "&|!%" "\\^#"]

    (
        ## Introduced in Layer 2.
        "\\==" | "\\!=" | "\\<=" | "\\>=" | "\\<" | "\\>"
    |
        ## Introduced in Layer 2.
        "&&&" | "|||" | "^^^" | "!!!" | "<<<" | ">>>" | "..!"
    |
        ## Introduced in Layer 2.
        "==" | "!=" | "<=" | ">=" | "??" | "**" | "//" | "%%"
    |
        "->" | ":=" | "::" | ".." | "<>" | "@@"
    |
        ## These are introduced in Layer 1.
        "{:" | ":}"
    |
        ## Note: We check this error condition here instead of in the
        ## comment parsing code, because comments get parsed as whitespace,
        ## which gets ignored. Rather than changing to not-quite-ignore
        ## whitespace -- which would be messy -- we instead notice `#:` here
        ## where we're parsing other punctuation. This is clean but a little
        ## non-obvious, hence this comment.
        "#:" .*
        { @error("Unterminated comment.") }
    |
        ## Single-character punctuation / operator.
        .
    )
:};

## Parses a single decimal digit (or spacer), returning its value.
def tokDecDigit = {:
    ch = ["_" "0".."9"]
    { intFromDigitChar(ch) }
:};

## Parses an integer literal.
def tokInt = {:
    ## Note: Introduced in Layer 2.
    &"0" ## Avoid calling the rule unless we know it will match.
    %tokInt2
|
    digits = tokDecDigit+
    { @int(intFromDigitList(10, digits)) }
:};

## Parses a run of regular characters or an escape / special sequence,
## inside a quoted string.
##
## **Note:** Additional rules for string character parsing are defined
## in Layer 2.
def tokStringPart = {:
    (
        chars = [! "\\" "\"" "\n"]+
        { $Peg::stringFromTokenList(chars) }
    )
|
    ## This is the rule that ignores spaces after newlines.
    "\n"
    " "*
    { "\n" }
|
    "\\"
    (
        "\\" { "\\" } |
        "\"" { "\"" } |
        "n"  { "\n" } |
        "r"  { "\r" } |
        "t"  { "\t" } |
        "0"  { "\0" }
    )
|
    ## Introduced in Layer 2.
    %tokStringPart2
:};

## Parses a quoted string.
def tokString = {:
    "\""
    parts = tokStringPart*

    (
        "\""
        { processStringParts(parts) }
    |
        { @error("Unterminated string literal.") }
    )
:};

## Parses an identifier (in the usual form). This also parses keywords.
def tokIdentifier = {:
    one = ["_" "$" "a".."z" "A".."Z"]
    rest = ["_" "$" "a".."z" "A".."Z" "0".."9"]*

    {
        def string = $Peg::stringFromTokenList([one, rest*]);
        ifValueOr { get(KEYWORDS, string) }
            { @identifier(string) }
    }
:};

## Parses the quoted-string identifier form.
def tokQuotedIdentifier = {:
    "\\"
    s = tokString

    { @identifier(dataOf(s)) }
:};

## "Parses" an unrecognized character. This also consumes any further
## characters on the same line, in an attempt to resynch the input.
def tokError = {:
    badCh = .
    [! "\n"]*

    {
        def msg = cat("Unrecognized character: ", get_typeName(badCh));
        @error(msg)
    }
:};

## Parses an arbitrary token or error.
def tokToken = {:
    tokString | tokIdentifier | tokQuotedIdentifier
|
    ## This needs to be listed after the quoted identifier rule, to
    ## prevent `\"...` from being treated as a `\` token followed by
    ## a string.
    tokPunctuation
|
    ## This needs to be listed after the identifier rule, to prevent
    ## an identifier-initial `_` from triggering this rule.
    tokInt
|
    tokError
:};

## Parses a file of tokens, yielding a list of them.
def tokFile = {:
    tokens = (tokWhitespace? tokToken)*
    tokWhitespace?

    { tokens }
:};


##
## Layer 2 definitions
##
## These are all specific to parsing Layer 2 (and higher).
##

## Map from opening tokens to corresponding closers. Used when parsing
## string interpolations.
def OPEN_CLOSE_MAP = {
    @@"(":  @@")",
    @@"[":  @@"]",
    @@"{":  @@"}",
    @@"{:": @@":}"
};

## Processes a list of `stringPart` elements, yielding a literal `string`
## token, an `interpolatedString`, or an `error`.
fn processStringParts(parts) {
    ## Handle the empty string as a special case.
    ifIs { eq(parts, []) }
        { return @string("") };

    ## Coalesce adjacent literal strings in `parts`. The result is an
    ## `elems` list consisting of alternating multi-character strings and
    ## interpolation maps.
    var elems = [];
    $Generator::filterPump(parts) { part /next ->
        ## If there is an error part (represented as an `@error` token),
        ## return it.
        ifIs { hasType(part, @@error) }
            { return part };

        ifIs { eq(elems, []) }
            {
                ## First element. No possibility to coalesce, so special-case
                ## it.
                elems := [part];
                yield /next
            };

        ifIs { hasType(part, String) }
            {
                def lastElem = $Sequence::nthFromEnd(elems, 0);
                ifIs { hasType(lastElem, String) }
                    {
                        ## The last item in `elems` and the current `part`
                        ## are both strings. Combine them.
                        elems := [
                            $Sequence::sliceExclusive(elems, 0)*,
                            cat(lastElem, part)
                        ];
                        yield /next
                    }
            };

        elems := [elems*, part]
    };

    ## Check for a simple result.
    ifIs { eq(get_size(elems), 1) }
        {
            ifValue { hasType(elems*, String) }
                { elem ->
                    ## There's only one element, and it's a string. So,
                    ## we have a simple non-interpolated string. Return it.
                    return @string(elem)
                }
        };

    ## At this point, we have a valid interpolation.
    return @interpolatedString(elems)
};

## Parses a `#: ... :#` comment, with nesting.
tokMultilineComment := {:
    "#:"

    (
        %tokMultilineComment
    |
        [! ":"]
    |
        ":" !"#"
    )*

    ":#"
:};

## Parses a single binary digit (or spacer), returning its value.
def tokBinDigit = {:
    ch = ["_01"]
    { intFromDigitChar(ch) }
:};

## Parses a single hexadecimal digit (or spacer), returning its value.
def tokHexDigit = {:
    ch = ["_" "0".."9" "a".."f" "A".."F"]
    { intFromDigitChar(ch) }
:};

## Parses an integer literal.
tokInt2 := {:
    "0x"
    digits = tokHexDigit+
    { @int(intFromDigitList(16, digits)) }
|
    "0b"
    digits = tokBinDigit+
    { @int(intFromDigitList(2, digits)) }
:};

## Parses a single hexadecimal character. This is called during `\x...;`
## parsing.
def tokHexChar = {:
    digits = tokHexDigit+
    { toString(intFromDigitList(16, digits)) }
:};

## Parses a single entity-named character. This is called during `\&...;`
## parsing.
def tokNamedChar = {:
    chars = ["a".."z" "A".."Z" "0".."9" "."]+
    {
        def name = $Peg::stringFromTokenList(chars);
        get(ENTITY_MAP, name)
    }
|
    "#x"
    digits = tokHexDigit+
    { toString(intFromDigitList(16, digits)) }
|
    "#"
    digits = tokDecDigit+
    { toString(intFromDigitList(10, digits)) }
:};

## Parses the inner portion of a string interpolation expression. This
## captures a list of inner tokens, ending when parentheses and braces are
## balanced out (and ignoring other could-be-matched delimiter tokens).
def tokStringInterpolation;
tokStringInterpolation := {:
    &["({["]
    open = tokToken
    expectClose = { get(OPEN_CLOSE_MAP, get_type(open)) }

    tokens = (
        tokWhitespace?
        (
            !["(){}[]"]
            !":}"
            t = tokToken
            { [t] }
        |
            %tokStringInterpolation
        )
    )*

    tokWhitespace?
    close = tokToken

    {
        ifIs { hasType(close, expectClose) }
            { [open, cat([], tokens*)*, close] }
    }
:};

## Additional `stringPart` definitions for Layer 2.
tokStringPart2 := {:
    "\\"
    (
        "/"  { "" }
    |
        ## This is the rule that ignores an escaped newline, followed by
        ## any number of spaces.
        "\n"
        " "*
        { "" }
    |
        "x"
        (
            one = tokHexChar
            rest = ("," tokHexChar)*
            ";"
            { cat(one, rest*) }
        |
            { @error("Invalid hexadecimal character literal.") }
        )
    |
        "&"
        (
            one = tokNamedChar
            rest = ("," tokNamedChar)*
            ";"
            { cat(one, rest*) }
        |
            { @error("Invalid named character literal.") }
        )
    |
        format = (
            "%"
            chars = ["0".."9" "a".."z" "A".."Z" "+-."]*
            { {format: $Peg::stringFromTokenList(chars)} }
        |
            { {} }
        )

        &["({["]

        (
            tokens = tokStringInterpolation
            { {format*, tokens} }
        |
            { @error("Unbalanced string interpolation.") }
        )
    |
        { @error("Invalid character literal escape sequence.") }
    )
:};


##
## Exported Definitions
##

## Documented in spec.
export fn tokenize(programText) {
    return $Peg::apply(tokFile, programText)
};
